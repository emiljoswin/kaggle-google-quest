{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "1-notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pAiES5PovHW1"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "QSzcgS_HPR1s",
        "colab_type": "text"
      },
      "source": [
        "####  NOTES\n",
        ">1. Verified that the model class and original model definition via repeated functio calls have extremely close validation scores **(0.2998 for normal and 0.3042 class for class)** ie, they are probably same.\n",
        ">2. Added better code cleaning functionalities and mis-spelling corrections.\n",
        "\n",
        "\n",
        "####  TODO\n",
        ">1. There is text augmenter library https://github.com/makcedward/nlpaug/blob/master/example/textual_augmenter.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "kUOxsJFgPR1v",
        "colab_type": "text"
      },
      "source": [
        "# Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "h1fl-jZnUbY5",
        "outputId": "2effe1bf-823f-4067-ffee-4cf2bd081a2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "UqQkC_myUkuY",
        "colab": {}
      },
      "source": [
        "! cp -r drive/My\\ Drive/ml_projects/kaggle/google_quest/google-quest-challenge ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "vFM0QFyWVBz4",
        "colab": {}
      },
      "source": [
        "# !ls  drive/My\\ Drive/ml_projects/ml_datasets/nlp_pretrained/gensim-embeddings-dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "QeU5zk-WUpUd",
        "colab": {}
      },
      "source": [
        "! cp drive/My\\ Drive/ml_projects/ml_datasets/nlp_pretrained/gensim-embeddings-dataset/glove.twitter.27B.200d.gensim .\n",
        "! cp drive/My\\ Drive/ml_projects/ml_datasets/nlp_pretrained/gensim-embeddings-dataset/glove.twitter.27B.200d.gensim.vectors.npy ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz8M24SyXPHl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "5703568f-191b-41ed-b620-7cc09cf0ed53"
      },
      "source": [
        "! pip install iterative-stratification"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: iterative-stratification in /usr/local/lib/python3.6/dist-packages (0.1.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.22.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.17.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (0.14.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "WlJOhT__PR15",
        "colab_type": "text"
      },
      "source": [
        "# Start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "OyoGX4jUT3vb",
        "outputId": "fb215d5a-9233-47ee-d679-7a9928caa60d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
        "display(HTML(\"<style>.run_this_cell {display: block !important;} </style\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.container { width:95% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.run_this_cell {display: block !important;} </style"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "wLiLE7SaVWAn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1616ccad-b625-44af-d8dd-805040d9acab"
      },
      "source": [
        "%tensorflow_version 2.x "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "bQt1xaLzT3vd",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, concatenate, LSTM, Bidirectional, Embedding, add, Dense, Masking\n",
        "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "from tensorflow.keras import models, layers, optimizers\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Any\n",
        "\n",
        "from collections import Counter\n",
        "from tqdm import trange, tqdm, tnrange, tqdm_notebook\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-VLOssAXc5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "B9RydZYvaDiQ",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "nq9g_M92Vgq1",
        "outputId": "96e18d9f-bdbf-4207-abfe-7002a84165af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgCrj99iVk81",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e7282d62-c8c7-4309-b3d7-f2b76b224ce4"
      },
      "source": [
        "np.seterr(divide='ignore', invalid='ignore')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "lc7FPfs4T3vg",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "heading_collapsed": true,
        "hidden": true,
        "id": "1TGVOD8XT3vj"
      },
      "source": [
        "### Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "O9jpKB9dPR2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\xa0', '\\t',\n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "6Aq9OA4hT3vj",
        "colab": {}
      },
      "source": [
        "def clean_text(x):\n",
        "\n",
        "    x = str(x)\n",
        "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^`{|}~' + '“”’':\n",
        "        x = x.replace(punct, f' {punct}')\n",
        "   \n",
        "    for punct in '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—_':\n",
        "        x = x.replace(punct, f' {punct}')\n",
        "        \n",
        "    for punct in puncts:\n",
        "        x = x.replace(punct, f' {punct}')\n",
        " \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "la2rU5NsT3vl",
        "colab": {}
      },
      "source": [
        "def _get_mispell(mispell_dict):\n",
        "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
        "    return mispell_dict, mispell_re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "MoFpBlmiT3vo",
        "colab": {}
      },
      "source": [
        "mispell_dict = {\"usepackage\" : \"use package\",\n",
        "                'instrumentsettingsid':'instrumental settings id',\n",
        "                'RippleShaderProgram' : 'ripple shader program',\n",
        "                'ShaderProgramConstants':'shader program constants',\n",
        "                'storedElements':'stored elements',\n",
        "                'stackSize' : 'stack size',\n",
        "                '_':' ',\n",
        "                \"aren't\" : \"are not\",\n",
        "                \"can't\" : \"cannot\",\n",
        "                \"couldn't\" : \"could not\",\n",
        "                \"couldnt\" : \"could not\",\n",
        "                \"didn't\" : \"did not\",\n",
        "                \"doesn't\" : \"does not\",\n",
        "                \"doesnt\" : \"does not\",\n",
        "                \"don't\" : \"do not\",\n",
        "                \"hadn't\" : \"had not\",\n",
        "                \"hasn't\" : \"has not\",\n",
        "                \"haven't\" : \"have not\",\n",
        "                \"havent\" : \"have not\",\n",
        "                \"he'd\" : \"he would\",\n",
        "                \"he'll\" : \"he will\",\n",
        "                \"he's\" : \"he is\",\n",
        "                \"i'd\" : \"I would\",\n",
        "                \"i'd\" : \"I had\",\n",
        "                \"i'll\" : \"I will\",\n",
        "                \"i'm\" : \"I am\",\n",
        "                \"isn't\" : \"is not\",\n",
        "                \"it's\" : \"it is\",\n",
        "                \"it'll\":\"it will\",\n",
        "                \"i've\" : \"I have\",\n",
        "                \"let's\" : \"let us\",\n",
        "                \"mightn't\" : \"might not\",\n",
        "                \"mustn't\" : \"must not\",\n",
        "                \"shan't\" : \"shall not\",\n",
        "                \"she'd\" : \"she would\",\n",
        "                \"she'll\" : \"she will\",\n",
        "                \"she's\" : \"she is\",\n",
        "                \"shouldn't\" : \"should not\",\n",
        "                \"shouldnt\" : \"should not\",\n",
        "                \"that's\" : \"that is\",\n",
        "                \"thats\" : \"that is\",\n",
        "                \"there's\" : \"there is\",\n",
        "                \"theres\" : \"there is\",\n",
        "                \"they'd\" : \"they would\",\n",
        "                \"they'll\" : \"they will\",\n",
        "                \"they're\" : \"they are\",\n",
        "                \"theyre\":  \"they are\",\n",
        "                \"they've\" : \"they have\",\n",
        "                \"we'd\" : \"we would\",\n",
        "                \"we're\" : \"we are\",\n",
        "                \"weren't\" : \"were not\",\n",
        "                \"we've\" : \"we have\",\n",
        "                \"what'll\" : \"what will\",\n",
        "                \"what're\" : \"what are\",\n",
        "                \"what's\" : \"what is\",\n",
        "                \"what've\" : \"what have\",\n",
        "                \"where's\" : \"where is\",\n",
        "                \"who'd\" : \"who would\",\n",
        "                \"who'll\" : \"who will\",\n",
        "                \"who're\" : \"who are\",\n",
        "                \"who's\" : \"who is\",\n",
        "                \"who've\" : \"who have\",\n",
        "                \"won't\" : \"will not\",\n",
        "                \"wouldn't\" : \"would not\",\n",
        "                \"you'd\" : \"you would\",\n",
        "                \"you'll\" : \"you will\",\n",
        "                \"you're\" : \"you are\",\n",
        "                \"you've\" : \"you have\",\n",
        "                \"'re\": \" are\",\n",
        "                \"wasn't\": \"was not\",\n",
        "                \"we'll\":\" will\",\n",
        "                \"didn't\": \"did not\",\n",
        "                \"tryin'\":\"trying\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "26ypEMSLT3vq",
        "colab": {}
      },
      "source": [
        "mispellings, mispellings_re = _get_mispell(mispell_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "7c_TchcCT3vs",
        "colab": {}
      },
      "source": [
        "def replace_typical_misspell(text):\n",
        "    def replace(match):\n",
        "        return mispellings[match.group(0)]\n",
        "\n",
        "    return mispellings_re.sub(replace, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "heading_collapsed": true,
        "hidden": true,
        "id": "CR8RjZmpT3vu"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "JZ2FDG8MT3vv",
        "colab": {}
      },
      "source": [
        "# train_loc = '/kaggle/input/google-quest-challenge/train.csv'\n",
        "# test_loc= '/kaggle/input/google-quest-challenge/test.csv'\n",
        "\n",
        "# train_loc = '../google-quest-challenge/train.csv'\n",
        "# test_loc= '../google-quest-challenge/test.csv'\n",
        "\n",
        "train_loc = 'google-quest-challenge/train.csv'\n",
        "test_loc= 'google-quest-challenge/test.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_loc)\n",
        "test_df = pd.read_csv(test_loc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "heading_collapsed": true,
        "hidden": true,
        "id": "BFcJKnZ9T3vy"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "SonttmKnT3vy",
        "colab": {}
      },
      "source": [
        "train_df[\"question_body\"] = train_df[\"question_body\"].apply(clean_text)\n",
        "train_df[\"question_body\"] = train_df[\"question_body\"].apply(replace_typical_misspell)\n",
        "\n",
        "train_df[\"question_title\"] = train_df[\"question_title\"].apply(clean_text)\n",
        "train_df[\"question_title\"] = train_df[\"question_title\"].apply(replace_typical_misspell)\n",
        "\n",
        "train_df[\"category\"] = train_df[\"category\"].apply(clean_text)\n",
        "train_df[\"category\"] = train_df[\"category\"].apply(replace_typical_misspell)\n",
        "\n",
        "train_df[\"answer\"] = train_df[\"answer\"].apply(clean_text)\n",
        "train_df[\"answer\"] = train_df[\"answer\"].apply(replace_typical_misspell)\n",
        "\n",
        "\n",
        "test_df[\"question_body\"] = test_df[\"question_body\"].apply(clean_text)\n",
        "test_df[\"question_body\"] = test_df[\"question_body\"].apply(replace_typical_misspell)\n",
        "\n",
        "test_df[\"question_title\"] = test_df[\"question_title\"].apply(clean_text)\n",
        "test_df[\"question_title\"] = test_df[\"question_title\"].apply(replace_typical_misspell)\n",
        "\n",
        "test_df[\"category\"] = test_df[\"category\"].apply(clean_text)\n",
        "test_df[\"category\"] = test_df[\"category\"].apply(replace_typical_misspell)\n",
        "\n",
        "test_df[\"answer\"] = test_df[\"answer\"].apply(clean_text)\n",
        "test_df[\"answer\"] = test_df[\"answer\"].apply(replace_typical_misspell)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "I7vq6-EDT3v0",
        "colab": {}
      },
      "source": [
        "question_body = train_df['question_body']\n",
        "answer = train_df['answer']\n",
        "question_title = train_df[\"question_title\"]\n",
        "category = train_df[\"category\"]\n",
        "\n",
        "question_body_test = test_df['question_body']\n",
        "answer_test = test_df['answer']\n",
        "question_title_test = test_df[\"question_title\"]\n",
        "category_test = test_df[\"category\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "lmPgad44T3v2",
        "colab": {}
      },
      "source": [
        "target = train_df[train_df.columns[-30:]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhUaD_G_qDBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_cols = train_df.columns[-30:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "heading_collapsed": true,
        "hidden": true,
        "id": "KRryeoqAT3v5"
      },
      "source": [
        "### Creating Tokenziers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "DJceUF8qT3v6",
        "colab": {}
      },
      "source": [
        "all_text = pd.concat([\n",
        "                    train_df['question_body'],\n",
        "                     train_df['answer'],\n",
        "                     test_df['question_body'],\n",
        "                     test_df['answer'],\n",
        "                     train_df[\"question_title\"],\n",
        "                     train_df[\"category\"],\n",
        "                     test_df[\"question_title\"],\n",
        "                     test_df[\"category\"]\n",
        "                    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "vk_RL_VvT3v7",
        "outputId": "dd546c5e-ca1a-4c77-9f19-1f0b54a5a9eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_df), len(test_df), 6079*4 + 476 * 4, len(all_text)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6079, 476, 26220, 26220)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "TivBnUFgT3v-",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=1000000, lower=False,filters='')\n",
        "\n",
        "tokenizer.fit_on_texts(all_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "J1KJ9QHuT3wA",
        "outputId": "ed6f3cc8-bea0-400f-96fb-606fea4f0aa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(tokenizer.word_counts), tokenizer.word_index['the'], tokenizer.word_index['ABC']"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100498, 4, 8783)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "Whxhk7tuT3wC",
        "colab": {}
      },
      "source": [
        "question_body = tokenizer.texts_to_sequences(question_body)\n",
        "answer = tokenizer.texts_to_sequences(answer)\n",
        "question_title = tokenizer.texts_to_sequences(question_title)\n",
        "category = tokenizer.texts_to_sequences(category)\n",
        "\n",
        "question_body_test = tokenizer.texts_to_sequences(question_body_test)\n",
        "answer_test = tokenizer.texts_to_sequences(answer_test)\n",
        "question_title_test = tokenizer.texts_to_sequences(question_title_test)\n",
        "category_test = tokenizer.texts_to_sequences(category_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "BzYwGh5XT3wE",
        "outputId": "a7d810dc-8b18-4db2-f5f7-76faae981c4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(question_body), len(question_body[0])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6079, 174)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "Eh5XnfXvT3wJ",
        "colab": {}
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "maxlen = 245"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "cbcETLGOPR24",
        "colab_type": "text"
      },
      "source": [
        "### Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "mVirIudhT3wM",
        "colab": {}
      },
      "source": [
        "question_body = pad_sequences(question_body, padding='post', maxlen=maxlen)\n",
        "answer = pad_sequences(answer, padding='post', maxlen=maxlen)\n",
        "question_title = pad_sequences(question_title, padding='post', maxlen=maxlen)\n",
        "category = pad_sequences(category, padding='post', maxlen=maxlen)\n",
        "\n",
        "\n",
        "question_body_test = pad_sequences(question_body_test, padding='post', maxlen=maxlen)\n",
        "answer_test = pad_sequences(answer_test, padding='post', maxlen=maxlen)\n",
        "question_title_test = pad_sequences(question_title_test, padding='post', maxlen=maxlen)\n",
        "category_test = pad_sequences(category_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "hidden": true,
        "id": "OWMWifotT3wO"
      },
      "source": [
        "### Build embedding matrix for the full dataset\n",
        "\n",
        "Gensim word vectors can be found here https://www.kaggle.com/iezepov/gensim-embeddings-dataset\n",
        "\n",
        "All UNK tokens will be 0s. This is different from the NLP project, where the UNK tokens are the average of the matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "YWyKvf9JT3wO",
        "colab": {}
      },
      "source": [
        "emb_size = 200\n",
        "\n",
        "def build_matrix(word_index, path):\n",
        "    embedding_index = KeyedVectors.load(path, mmap='r')\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, emb_size))\n",
        "    for word, i in word_index.items():\n",
        "        for candidate in [word, word.lower()]:\n",
        "            if candidate in embedding_index:\n",
        "                embedding_matrix[i] = embedding_index[candidate]\n",
        "                break\n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "R91apGGLPR2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EMBEDDING_FILES = ['/Users/emiljoswin/study/ml-projects/NLP/pre-trained-weights/glove/glove.840B.300d.txt']\n",
        "\n",
        "# glove_file = datapath(EMBEDDING_FILES[0])\n",
        "\n",
        "# tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
        "\n",
        "# _ = glove2word2vec(glove_file, \"test_word2vec.txt\")\n",
        "\n",
        "# model = KeyedVectors.load_word2vec_format(tmp_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "MK0uZLtXT3wQ",
        "outputId": "e552aba0-13b8-4a37-c783-f1c66c0f9596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# EMBEDDING_FILES = ['/Users/emiljoswin/study/playground/jupyter_notebooks/NLP/crawl-300d-2M.gensim', \n",
        "#                   '/Users/emiljoswin/study/playground/jupyter_notebooks/NLP/glove.840B.300d.gensim']\n",
        "\n",
        "# EMBEDDING_FILES = ['/kaggle/input/gensim-word-embeddings/glove.twitter.27B.200d.gensim']\n",
        "\n",
        "EMBEDDING_FILES = ['glove.twitter.27B.200d.gensim']\n",
        "\n",
        "\n",
        "# EMBEDDING_FILES = ['../../NLP/pre-trained-weights/gensim-embeddings-dataset/glove.twitter.27B.200d.gensim']\n",
        "# EMBEDDING_FILES = ['/Users/emiljoswin/study/ml-projects/NLP/pre-trained-weights/gensim-embeddings-dataset/glove.840B.300d.gensim']\n",
        "\n",
        "\n",
        "embedding_index = KeyedVectors.load(EMBEDDING_FILES[0], mmap='r')\n",
        "# embedding_index = KeyedVectors.load(EMBEDDING_FILES[1], mmap='r')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "8rIFw0osT3wS",
        "outputId": "120d6596-110e-41f6-f9bb-9f0397195b69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "m1 = build_matrix(tokenizer.word_index, EMBEDDING_FILES[0])\n",
        "# m2 = build_matrix(tokenizer.word_index, EMBEDDING_FILES[1])\n",
        "# m1.shape, m2.shape, \n",
        "m1.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100499, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "_8QlZPaQT3wY",
        "outputId": "73c3d9a5-a40d-417f-882d-e0a01e24663d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# embedding_matrix = np.concatenate([m1, m2], axis=-1)\n",
        "# embedding_matrix.shape\n",
        "\n",
        "embedding_matrix = m1\n",
        "embedding_matrix.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100499, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "heading_collapsed": true,
        "id": "UenY-gXKCfgV"
      },
      "source": [
        "# Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "ChjGCi33C39o",
        "outputId": "2a3bf4fc-af04-4901-acd3-1867be5b6b05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "X = concatenate([question_body, question_title, category, answer], 1)\n",
        "print(X.shape)\n",
        "Y = target.to_numpy()\n",
        "print(Y.shape)\n",
        "\n",
        "test_original = concatenate([question_body_test, question_title_test, category_test, answer_test], 1)\n",
        "print(test_original.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6079, 980)\n",
            "(6079, 30)\n",
            "(476, 980)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "0SfJadHkPR3I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0776b9ee-e1ea-4c22-b98a-c21cdc01a885"
      },
      "source": [
        "total = 6079\n",
        "val_size = 700\n",
        "\n",
        "p = np.random.permutation(list(range(total)))\n",
        "\n",
        "val_idx = p[:val_size]\n",
        "train_idx = p[val_size:]\n",
        "\n",
        "train_x = tf.gather(X, train_idx)\n",
        "train_y = tf.gather(Y, train_idx)\n",
        "\n",
        "val_x = tf.gather(X, val_idx)\n",
        "val_y = tf.gather(Y, val_idx)\n",
        "\n",
        "print(train_x.shape, train_y.shape, val_x.shape, val_y.shape)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5379, 980) (5379, 30) (700, 980) (700, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "2xTUSau1PR3K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "91c8b524-49ad-4bff-fcb5-05cd7258668e"
      },
      "source": [
        "print(val_idx)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 288 2811 4785 4418 2758 5392 4497 4225 3837 3285 3834 5151 6077 2574\n",
            " 2270 2820 1345  653 2673  855 4191 4250 2951 3494   80 4967 4104  400\n",
            " 1761 3482  718 5350 5584 2032 3443 3038 4773 1127 5157 4700 5721 2053\n",
            " 1358  319  209 5731 1155  238 1614 4185 2174 3422 5404 2659 4883 2865\n",
            " 1487 4951 1543 4887 4596 4871 3300 2395 4812  113 1486 4993 3791 2036\n",
            " 3520 3832 2821 3857 2809 5411 5365 5158 5409 2976  975 3603 4333  945\n",
            " 1716 6035 4114  432  520 5487 2099 1575 2251 1298 1413 2166 1060  483\n",
            "  293 5491 2285 3064  442 4904 3729 4448 5918 5597 2170  670 3724 1444\n",
            " 3546 5028  395 1685 4262 5309 3806 2078 1106 2601 3307  281 5471  572\n",
            " 2165 4463 5980 2076 4532 5210 2812 5454 2204 2539 4390 4243 4394 3697\n",
            " 3170 1872 5698 1343  360 4576 2714 2766 5375 3316   95 4275 4663 4401\n",
            "  514 2264 1221 3394 3461 3111 1363  335  611 5178  986 4220 5542 5593\n",
            " 3485 3351 4828 4896  423 2410 3741 4359 4382 1812 5133 2469   86 3110\n",
            " 5783 5797 5059 4314 1617 5223 2253  321 1905 1253 2496 2411 5569 3921\n",
            " 2180 2653 5919 4927 1653 5973 1941 3598 2231 5385  727 3954 6052 4851\n",
            " 1282 5836 5079 2961  756 4340 5473 4164 1123 3811 2823  650 3141 4670\n",
            "  575  748  803 2468 5481 5861 5405  277 1373 5889 5015 1110 5200 2604\n",
            "  429  864  177 1690 5038  416 3675 4411 1117 3118 2991 1483  388 2957\n",
            " 4729   39 3021 1440 1044 5302 1007 2704 4279  546 4047 6064 3238  199\n",
            " 4029 2199 3490 1417 1354 5561  125 4063 1437 1609 2634 1426 2215 2875\n",
            " 2827 5908 1781  900 5870 3155 5695 4675 5106 3435 5537 5239 2678 4235\n",
            " 2573 1596 3425 2942 1059 5825 1448 2301 1192 2753 2156 5342 1817 3474\n",
            " 4492  445 3450 1013  867 1613 2310 2275 3172 1255  488 6018 2266  453\n",
            " 1784 5400 2200 2868 1036 1077 2205 2289 5890 4209 3190 3318 3221 4458\n",
            " 2833 4079 1264 3071 2068 1577 1809 3469 4150 1003 4369 5997   24  914\n",
            " 1759 1010 5303  775 1908 5476 2681  750 2872 5951  141 1349 5189 1780\n",
            " 2591 3658 5776 3152 4809 4420 1805 3514  245  884  992 2110  684 3217\n",
            " 1774 5479 5904 2654 4177 1776  822 2975 2530 1507  485 3518 3286   38\n",
            " 2094 1892 5938 5153  276 1135 4701 5132 5524 4956  376 2565  410 1525\n",
            " 4205 3585 3822  263 4719 2272 3926 2135 2805 5527 5160 3150 3235 4480\n",
            " 2931 5602 4975 5055 5553 1414 1562  264  702  892 2407  117 1213 5917\n",
            " 4437 4339 5556 4244 3637 3948 4368  462 4903 1366 2829 4397 3209 1055\n",
            " 3517 5296 2246  359 5996 5138  710 4044  732 5782 5147  539 2083  425\n",
            " 4582 5330 2663 3448 3605 2567 2448 5310 2746 1917 5590 5780  338 5850\n",
            " 2169  861 1931 5040 2084 1950  646  566 5156 4758 3418 5021 1976 2803\n",
            "  839 2348 4565 5503 1741  331 1131 2333   88 5884  289 1484 4482 2909\n",
            "   82 3845 4171  112 4449 2655 5130 1553 5093  791 5749  977 3497 5262\n",
            " 1571 3287 4409 5730 1469  162    1 3008 5376 4350 3945 4948 2516 5141\n",
            " 6037 2037  570  478 5204 2388 1657 1827 4178 4738  126 6053 3032 3815\n",
            " 4445 6047 1150 2108 1474 5943  260 3762 2092 2635 3124 1988 1395 2194\n",
            " 3756 4734 4844  296 4310  172 1813 5683 2900 2247 2553 3947 2488 3060\n",
            " 3389  511 1122 3199 3382 4607  916 3989 1902 1389 5673 4924 2886 5317\n",
            "  671 2569  531 1834 4571 1934  925 2282 3554 4255 1166  504 5321 3978\n",
            " 4739 5029 1959 3355 2953  711 4517 4703 4010  865 1356  223 5035 3526\n",
            " 1963 4410 4742 3001  241 4496 1782 3908 2222 4704 5539 1374 1701 5600\n",
            " 1049 2545 4706 5521 1627 2538 3253  470 5541   56 2345 1597  736 3084\n",
            " 1336 3085 2593 2607 5417 2369 5963 3900 3747  475 4595 4194 1568 4226\n",
            " 3116 5364 3770 3265 4432  187 3974 1581 5581  996 6044 4163 4067 5913\n",
            " 5328 2370 1330 3661  933 4688 4056 5275 4961 3169 2141 2506 3347 3099\n",
            "  863  761 4147 4737 4686 3239  683 3354 3511 1194 3390 5255 4317 2891]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "uCJo4gTIPR3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_x = tf.convert_to_tensor(val_x)\n",
        "val_y = tf.convert_to_tensor(val_y)\n",
        "\n",
        "train_x = tf.convert_to_tensor(train_x)\n",
        "train_y = tf.convert_to_tensor(train_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "JrNYBEpUPR3N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "97f9b821-3bd3-4079-cbd9-f4b075eb12fb"
      },
      "source": [
        "train_x.shape, train_y.shape, val_x.shape, val_y.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([5379, 980]),\n",
              " TensorShape([5379, 30]),\n",
              " TensorShape([700, 980]),\n",
              " TensorShape([700, 30]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "bdZJOGPrPR3P",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "EWIA7hdbpcEZ",
        "colab": {}
      },
      "source": [
        "from scipy.stats import spearmanr\n",
        "import math\n",
        "\n",
        "def calculate_mean_spearman_co(a, b):\n",
        "    \"\"\"\n",
        "        Calculates spearmans correlation coefficient \n",
        "        https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient\n",
        "        \n",
        "        In the scipy implementation if you pass x and y and if one of them is composed of all same values,\n",
        "        then it returns nan. I am changing those nans to zeros\n",
        "    \"\"\"\n",
        "    # print(a.shape, b.shape, len(a.shape))\n",
        "    # print(a, b)\n",
        "    if len(a.shape) == 1:\n",
        "        return 0\n",
        "\n",
        "    if a.shape[0] is None and a.shape[1] is None:\n",
        "         return 0\n",
        "         \n",
        "    ans = [spearmanr(a[:, i], b[:, i])[0] for i in range(a.shape[1])]\n",
        "\n",
        "    ans = [0. if math.isnan(elem) else elem for elem in ans]\n",
        "    return np.mean(ans), ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d_-i384iDTgz"
      },
      "source": [
        "# Extras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U6Xe49SJDUfV",
        "colab": {}
      },
      "source": [
        "# LSTM_UNITS = 64\n",
        "# DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
        "# BATCH_SIZE = 128\n",
        "# EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kG5WumZ7DXL-",
        "colab": {}
      },
      "source": [
        "# words = Input(shape=(None,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x1W2klM3DfXF",
        "colab": {}
      },
      "source": [
        "# x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
        "# x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
        "# x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
        "\n",
        "# hidden = concatenate([\n",
        "#         GlobalMaxPooling1D()(x),\n",
        "#         GlobalAveragePooling1D()(x),\n",
        "#     ])\n",
        "\n",
        "# hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
        "# hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q-z0nvBCDhxU",
        "colab": {}
      },
      "source": [
        "# result = Dense(30, activation='sigmoid')(hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wzJcTMZODkJ9",
        "colab": {}
      },
      "source": [
        "# model2 = Model(inputs=words, outputs=[result])\n",
        "# model2.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RD9xp-VMDmS0",
        "colab": {}
      },
      "source": [
        "# model2.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CpGOlvcPR3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model2.fit(\n",
        "#             train_x, train_y,\n",
        "#             batch_size=256,\n",
        "#             epochs=20,\n",
        "#             verbose=1,\n",
        "#         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szvAEhB1PR3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# val_predictions = model2.predict(val_x)\n",
        "# m, _ = calculate_mean_spearman_co(val_predictions, val_y)\n",
        "# print(\"val_spearman\", m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u-Xg8fMPR3j",
        "colab_type": "text"
      },
      "source": [
        "# Model as a class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UVj5VpcPR3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LSTM_UNITS = 64\n",
        "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
        "BSZ = 32\n",
        "EPOCHS = 10\n",
        "OUTPUT_CLASSES = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYpazvSQPR3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(models.Model):\n",
        "    def __init__(self, embedding_matrix):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.mask = Masking(mask_value=0)\n",
        "        self.emb = Embedding(*embedding_matrix.shape, weights=[embedding_matrix])\n",
        "        self.b1 = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))\n",
        "        self.b2 = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))\n",
        "#         self.attn_weight = tf.Variable(tf.random.normal((LSTM_UNITS*2, 1)))\n",
        "        self.g1 = GlobalMaxPooling1D()\n",
        "        self.g2 = GlobalAveragePooling1D()\n",
        "        \n",
        "        self.d1 = Dense(DENSE_HIDDEN_UNITS, activation='relu')\n",
        "        self.d2 = Dense(DENSE_HIDDEN_UNITS, activation='relu')\n",
        "        self.linear = Dense(OUTPUT_CLASSES, activation='sigmoid')\n",
        "        \n",
        "    def attn(self, inputs):\n",
        "        \"\"\"\n",
        "            inputs: [bsz x 980 x LSTM_UNITS*2]\n",
        "            output: [bsz x LSTM_UNITS]\n",
        "        \"\"\"\n",
        "        \n",
        "        # (1 x 128) (bsz x seq x 128 )T  => (2 x seq)\n",
        "        check = tf.einsum('ij,kjm->kim',\n",
        "                          tf.transpose(self.attn_weight), \n",
        "                          tf.transpose(inputs, [0, 2, 1])) # (bsz x 1 x 980) - a score for each time step\n",
        "        \n",
        "        check = tf.squeeze(check) # (bsz x 980)\n",
        "        check = tf.nn.softmax(check, axis=1) # attention weight (bsz x 980)\n",
        "        \n",
        "        #(bsz x seq x 128)T (bsz x seq) => (bsz x 128)\n",
        "        check = tf.einsum('ijk,ki->ij', \n",
        "                          tf.transpose(inputs, [0, 2, 1]), \n",
        "                          tf.transpose(check) ) # (bsz x LSTM_UNITS) - best of each 980 time-steps\n",
        "        \n",
        "        return check\n",
        "\n",
        "        \n",
        "        \n",
        "    def call(self, inputs):\n",
        "#         msk = inputs != 0\n",
        "        x = self.mask(inputs)\n",
        "        x = self.emb(x)\n",
        "        x = self.b1(x)\n",
        "        x = self.b2(x)\n",
        "#         x1 = self.attn(x) # (bsz x LSTM_UNITS*2)\n",
        "#         x2 = self.attn(x)\n",
        "        x = concatenate([self.g1(x), self.g2(x)])\n",
        "        \n",
        "        x = add([x, self.d1(x)])\n",
        "        x = add([x, self.d2(x)])\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52KLfUa7PR3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model3 = MyModel(embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcXqdSgjPR3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time \n",
        "# out = model3(train_x[0:2, :]) # Masking with Masking Class\n",
        "\n",
        "# \"\"\"\n",
        "# CPU times: user 2min 33s, sys: 3.18 s, total: 2min 36s\n",
        "# Wall time: 39.9 s\n",
        "# \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMIIdODWPR3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time \n",
        "# out = model3(train_x[0:2, :]) #Non masking\n",
        "# \"\"\"\n",
        "# CPU times: user 2min 32s, sys: 2.68 s, total: 2min 35s\n",
        "# Wall time: 38.9 s\n",
        "# \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "8nFqfOmLPR3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time \n",
        "# out = model3(train_x[0:2, :]) #Masking normally by passing explicit mask values to LSTM layers\n",
        "# \"\"\"\n",
        "# CPU times: user 3min 19s, sys: 3.48 s, total: 3min 22s\n",
        "# Wall time: 53.9 s\n",
        "\n",
        "# \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PHvcUfqPR3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# out.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-UJ1RzdPR3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model3.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqccRfTsQsMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SpearmanRhoCallback(Callback):\n",
        "    def __init__(self, training_data, validation_data, patience, model_name):\n",
        "        print(\"inside Init\")\n",
        "        self.x = training_data[0]\n",
        "        self.y = training_data[1]\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        \n",
        "        self.patience = patience\n",
        "        self.value = -1\n",
        "        self.bad_epochs = 0\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        y_pred_val = self.model.predict(self.x_val)\n",
        "#         rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n",
        "        rho_val = np.mean([ spearmanr(self.y_val[:, ind], y_pred_val[:, ind]).correlation for ind in range(y_pred_val.shape[1]) ])\n",
        "        print(\"\\n rho_val\", rho_val)\n",
        "        if rho_val >= self.value:\n",
        "            self.value = rho_val\n",
        "            self.bad_epochs = 0\n",
        "            print('save best weights')\n",
        "            self.model.save_weights(self.model_name)\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "            print('bad epochs = ',self.bad_epochs)\n",
        "        if self.bad_epochs >= self.patience:\n",
        "            print(\"Epoch %05d: early stopping Threshold -- load best weights\" % epoch)\n",
        "            try:\n",
        "                self.model.load_weights(self.model_name)\n",
        "            except:\n",
        "                print('could not load a model')\n",
        "            self.model.stop_training = True\n",
        "            \n",
        "#             \n",
        "        print('\\rval_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n",
        "        return rho_val\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAiES5PovHW1",
        "colab_type": "text"
      },
      "source": [
        "##### MISC code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyuRD9FVR_3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_x.shape, val_x.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbWY7FacWAik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NUM_FOLDS = 5\n",
        "# kf = MultilabelStratifiedKFold(n_splits = NUM_FOLDS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OYcIpiiYOkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Y.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DON4OXSSbQng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n",
        "# y = np.array([[0,0, 1], [0,0, 1], [0,1, 0], [0,1, 3], [1,1, 0], [1,1, 3], [1,0, 3], [1,0, 3]])\n",
        "\n",
        "# mskf = MultilabelStratifiedKFold(n_splits=2)\n",
        "\n",
        "# for train_index, test_index in mskf.split(X, y):\n",
        "#    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "#    X_train, X_test = X[train_index], X[test_index]\n",
        "#    y_train, y_test = y[train_index], y[test_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzZRsNfEX1eI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NUM_FOLDS = 5\n",
        "# kf = MultilabelStratifiedKFold(n_splits = NUM_FOLDS)\n",
        "\n",
        "# for i, (train_idx, val_idx) in enumerate(kf.split(X, Y)):\n",
        "#     print(len(train_idx), len(val_idx), train_idx[:5], val_idx[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5Ubb0Enb7cF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    model = MyModel(embedding_matrix)\n",
        "    model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdVraw1Ot49p",
        "colab_type": "text"
      },
      "source": [
        "### CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyKHT_SOSFbq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "d9790456-f17c-45ba-a1b0-52678b80bf23"
      },
      "source": [
        "NUM_FOLDS = 3\n",
        "kf = MultilabelStratifiedKFold(n_splits=NUM_FOLDS)\n",
        "all_spearmans = []\n",
        "\n",
        "def do_cv():\n",
        "    for i, (train_idx, val_idx) in enumerate(kf.split(X, Y)):\n",
        "        # print(len(train_idx), len(val_idx), train_idx[:5], val_idx[:5])\n",
        "        print(f'Fold: {i+1}')\n",
        "        model = create_model()\n",
        "        train_x = tf.gather(X, train_idx)\n",
        "        train_y = tf.gather(Y, train_idx)\n",
        "\n",
        "        val_x = tf.gather(X, val_idx)\n",
        "        val_y = tf.gather(Y, val_idx)\n",
        "\n",
        "        # print( train_x.shape,  train_y.shape, val_x.shape, val_y.shape) \n",
        "        model.fit(\n",
        "            x=train_x,\n",
        "            y=train_y,\n",
        "            batch_size=BSZ,\n",
        "            epochs=1,\n",
        "            verbose=1,\n",
        "            validation_data=(val_x, val_y),\n",
        "            callbacks=[SpearmanRhoCallback(training_data=(train_x, train_y), validation_data=(val_x, val_y),\n",
        "                                            patience=15, model_name=f'best_model_batch_1.h5')]\n",
        "        )\n",
        "\n",
        "        pred_val = model.predict(val_x)\n",
        "        spearmans = [ spearmanr(val_y[:, ind], pred_val[:, ind]).correlation for ind in range(pred_val.shape[1]) ]\n",
        "        all_spearmans.append(spearmans)\n",
        "\n",
        "\n",
        "do_cv()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold: 1\n",
            "inside Init\n",
            "Train on 4053 samples, validate on 2026 samples\n",
            "4032/4053 [============================>.] - ETA: 0s - loss: 0.0584 - mae: 0.1692\t rho_val 0.23771471054174845\n",
            "\n",
            "save best weights\n",
            "\n",
            "val_spearman-rho: 0.2377                                                                                                    \n",
            "4053/4053 [==============================] - 42s 10ms/sample - loss: 0.0584 - mae: 0.1691 - val_loss: 0.0537 - val_mae: 0.1562\n",
            "Fold: 2\n",
            "inside Init\n",
            "Train on 4051 samples, validate on 2028 samples\n",
            "4032/4051 [============================>.] - ETA: 0s - loss: 0.0585 - mae: 0.1689\t rho_val 0.23619459767448872\n",
            "\n",
            "save best weights\n",
            "\n",
            "val_spearman-rho: 0.2362                                                                                                    \n",
            "4051/4051 [==============================] - 42s 10ms/sample - loss: 0.0584 - mae: 0.1688 - val_loss: 0.0528 - val_mae: 0.1594\n",
            "Fold: 3\n",
            "inside Init\n",
            "Train on 4054 samples, validate on 2025 samples\n",
            "4032/4054 [============================>.] - ETA: 0s - loss: 0.0584 - mae: 0.1690\t rho_val 0.24517303688540942\n",
            "\n",
            "save best weights\n",
            "\n",
            "val_spearman-rho: 0.2452                                                                                                    \n",
            "4054/4054 [==============================] - 42s 10ms/sample - loss: 0.0584 - mae: 0.1690 - val_loss: 0.0539 - val_mae: 0.1581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Set7E7fxpLfO",
        "colab_type": "text"
      },
      "source": [
        "#### Results to compare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ8LpwKPpLUC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ec154900-4100-4686-cdf0-1d3f9c134fac"
      },
      "source": [
        "m = np.mean(all_spearmans, axis=1)\n",
        "print(f'Spearmans over folds: {m}')\n",
        "std = np.std(m)\n",
        "m = np.mean(m)\n",
        "print(f'Overal Spearmans: {m} +/ {std}')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spearmans over folds: [0.23771471 0.2361946  0.24517304]\n",
            "Overal Spearmans: 0.2396941150338822 +/ 0.003923571865333851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzukCcgmp7vc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "1f9e73c4-770c-4aff-ebe6-f1cba7ced1af"
      },
      "source": [
        "m = np.mean(all_spearmans, axis=0)\n",
        "print(f'Val score for each Labels')\n",
        "print(f'`````````````````````````')\n",
        "for i, t in enumerate(target_cols):\n",
        "    print(f'{m[i]}  : {t} ')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Val score for each Labels\n",
            "`````````````````````````\n",
            "0.2549821114388325  : question_asker_intent_understanding \n",
            "0.379585228916246  : question_body_critical \n",
            "0.3182242617309056  : question_conversational \n",
            "0.19851628194693202  : question_expect_short_answer \n",
            "0.1306886057633775  : question_fact_seeking \n",
            "0.30197352716717746  : question_has_commonly_accepted_answer \n",
            "0.290024284462556  : question_interestingness_others \n",
            "0.41444200215857685  : question_interestingness_self \n",
            "0.2890225803891104  : question_multi_intent \n",
            "0.012557416492219843  : question_not_really_a_question \n",
            "0.21374984738715141  : question_opinion_seeking \n",
            "0.24161426329566202  : question_type_choice \n",
            "0.18333031674464842  : question_type_compare \n",
            "0.10015711014976263  : question_type_consequence \n",
            "0.25892575543800006  : question_type_definition \n",
            "0.22842744970598008  : question_type_entity \n",
            "0.6579571885157994  : question_type_instructions \n",
            "0.21829199879756941  : question_type_procedure \n",
            "0.38799984963538886  : question_type_reason_explanation \n",
            "0.029935733186092578  : question_type_spelling \n",
            "0.3334831145955041  : question_well_written \n",
            "0.04862606163291885  : answer_helpful \n",
            "0.20154183343851426  : answer_level_of_information \n",
            "0.01880010110978285  : answer_plausible \n",
            "0.045206060700732346  : answer_relevance \n",
            "0.04813089258597525  : answer_satisfaction \n",
            "0.6631994963047814  : answer_type_instructions \n",
            "0.17298992039932481  : answer_type_procedure \n",
            "0.470940240387642  : answer_type_reason_explanation \n",
            "0.07749991653930068  : answer_well_written \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80hIcUKgqyeI",
        "colab_type": "text"
      },
      "source": [
        "### Final Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqXwK0qdPR33",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "32f852b6-7e7e-4f38-9669-a045e72e7d2b"
      },
      "source": [
        "# After computing the total epoch number\n",
        "model = create_model()\n",
        "model.fit(\n",
        "            X, tf.convert_to_tensor(Y),\n",
        "            batch_size=BSZ,\n",
        "            epochs=1,\n",
        "            verbose=1,\n",
        "        )"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6079 samples\n",
            "6079/6079 [==============================] - 46s 8ms/sample - loss: 0.0569 - mae: 0.1653\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f89b09a7630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEkt1SBMPR35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# val_predictions = model3.predict(val_x)\n",
        "# m, _ = calculate_mean_spearman_co(val_predictions, val_y)\n",
        "# print(\"val_spearman\", m) #0.3237 => gave 0.289"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "DkDN2Jf3PR36",
        "colab_type": "text"
      },
      "source": [
        "# Saving the predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "E9x0pk2zPR36",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d6dd6ed-dc8c-48eb-e371-cb26f07910f0"
      },
      "source": [
        "predictions = model.predict(test_original)\n",
        "print(predictions.shape)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(476, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "mwoUShc4PR39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loc = '/kaggle/input/google-quest-challenge/sample_submission.csv'\n",
        "loc = 'google-quest-challenge/sample_submission.csv'\n",
        "sub = pd.read_csv(loc)\n",
        "\n",
        "for col_index, col in enumerate(target_cols):\n",
        "    sub[col] = predictions[:, col_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "mkO7dGLxPR4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub.to_csv('submission.csv', index = False)\n",
        "\n",
        "# Overal Spearmans: 0.2396941150338822 +/ 0.003923571865333851"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}